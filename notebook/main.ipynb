{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from math import ceil\n",
    "from groq import Groq\n",
    "\n",
    "GROQ_API_KEY = \"API_KEY_HERE\"\n",
    "\n",
    "MP3_FILE_PATH = '../data/DS DATA101- Introduction to Data Industry Ders 4.mp3'\n",
    "OUTPUT_FOLDER = '../data/trimmed_audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_mp3(input_file, output_folder, max_size_mb=20, min_size_mb=10, overlap_seconds=10):\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_mp3(input_file)\n",
    "    \n",
    "    # Calculate file size in bytes\n",
    "    file_size = os.path.getsize(input_file)\n",
    "    \n",
    "    # Calculate number of parts\n",
    "    num_parts = ceil(file_size / (max_size_mb * 1024 * 1024))\n",
    "    \n",
    "    # Calculate part duration in milliseconds\n",
    "    part_duration = len(audio) // num_parts\n",
    "    \n",
    "    # Ensure part size is within constraints\n",
    "    while (part_duration * file_size / len(audio)) / (1024 * 1024) < min_size_mb:\n",
    "        num_parts -= 1\n",
    "        part_duration = len(audio) // num_parts\n",
    "    \n",
    "    # Convert overlap to milliseconds\n",
    "    overlap_ms = overlap_seconds * 1000\n",
    "    \n",
    "    # Trim audio into parts\n",
    "    parts = []\n",
    "    for i in range(num_parts):\n",
    "        start = max(0, i * part_duration - overlap_ms)\n",
    "        end = min(len(audio), (i + 1) * part_duration + overlap_ms)\n",
    "        part = audio[start:end]\n",
    "        parts.append(part)\n",
    "    \n",
    "    # Export parts\n",
    "    for i, part in enumerate(parts):\n",
    "        output_file = output_folder + f\"/output_part_{i+1}.mp3\"\n",
    "        part.export(output_file, format=\"mp3\")\n",
    "        print(f\"Exported {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported ../data/trimmed_audio/output_part_1.mp3\n",
      "Exported ../data/trimmed_audio/output_part_2.mp3\n",
      "Exported ../data/trimmed_audio/output_part_3.mp3\n"
     ]
    }
   ],
   "source": [
    "trim_mp3(MP3_FILE_PATH, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " En son hatırlıyorsanız arkadaşlar veri bilimi iş \n"
     ]
    }
   ],
   "source": [
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "model = 'whisper-large-v3'\n",
    "\n",
    "def audio_to_text(filepath):\n",
    "    \"\"\"\n",
    "    Transcribes the audio file located at the given filepath into text.\n",
    "\n",
    "    Parameters:\n",
    "    filepath (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "    str: The transcribed text from the audio file.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"rb\") as file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            file=(filepath, file.read()),\n",
    "            model=\"whisper-large-v3\",\n",
    "        )\n",
    "    return transcription.text\n",
    "\n",
    "filepath = OUTPUT_FOLDER + \"/output_part_1.mp3\"\n",
    "transcription_text = audio_to_text(filepath)\n",
    "print(transcription_text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an Educational Transcribed Content Editor specializing in Turkish Data Science lectures. Your task is to edit the provided transcription while adhering to the following guidelines:\n",
    "\n",
    "1. Maintain the conversational style and flow of the original lecture.\n",
    "2. Correct any typos, grammatical errors, or mistranslated words.\n",
    "3. Ensure technical terms related to Data Science are accurately represented.\n",
    "4. Preserve the speaker's original intent and tone.\n",
    "5. Make minimal changes necessary for clarity and correctness.\n",
    "6. The final transcription must be in Turkish.\n",
    "\n",
    "Original Transcription:\n",
    "{transcription}\n",
    "\n",
    "Please provide the edited version of the transcription below:\n",
    "\n",
    "Edited Transcription:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"transcription\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    api_key=GROQ_API_KEY\n",
    "    )\n",
    "pipeline = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Düzenlenmiş Transkript:\n",
      "En son hatırlıyorsanız arkadaşlar, veri bilimi iş akışı veya uygulama çerçevesi olarak tabir ettiğim kısımdan bahsetmiştik. Temelimizi problemi anlamak oluşturuyordu aslında. Yani veriden önce üzerinde çalışacağımız problemi anlamamız gerekiyordu. Daha sonra veri toplama, veri hazırlama, keşifsel veri analizi, analitik, optimizasyon derken zaten tam bir döngüden bahsetmiştik. Buradan sonra biraz bu adımların detaylarına gireceğiz. Bazıları gerçekten çok detaylı gidecek. Özellikle o detaylı kısımlarda birçok yerde beni şey derken bulacaksınız. İşte bundan ilerideki derste bahsedeceğiz falan gibi. Ama olabildiğince de kısa kısa aklınızda verebilecek şekilde oluşmasına yardımcı olmaya çalışacağım oralarda. Buradaki amacımız size overall bütün resmi sunmak diyebilirim.\n",
      "\n",
      "Şimdi buralara gelmeden önce biraz daha önce işlediğimiz şeylere bir bakalım istiyorum. Biraz vakit de geçti çünkü. Burada bazı olmak çalışmalardan bahsetmiştik. Burada hatta sizlerden de örnekler almıştım diye hatırlıyorum. Yine burada da benzer örneklerimiz vardı. Bu veri stratejilerinden bahsetmiştik. İşte descriptive, exploratory, inferential, predictive, causal ve prescriptive analytics'ten bahsetmiştik. Zaten bilgi piramidinin üzerinden anlatmayı sevdiğimden bahsetmiştim. Bence oldukça güzel bir metafor. Hem daha çok böyle envisioning information, bu infografiklerin şeyi, ne derler, ilk başlangıcında ortaya atılan aslında bir metafor bu, bilgi piramidi. Ben onu, açıkçası biraz böyle data science tarafına doğru, şey, ne derler, uyarlamıştım aslında. Önceki adımlarla beraber. Veritablerinden bahsettik. Veri kaynaklarından bahsettik. Veri bilim nedir'den bahsettik. O buralarda bir değer, bir business bölümü yaratmaya çalışıyoruz. Yani bir iş değeri yaratmaya çalışıyoruz. Burada yaratacağınız iş değerinin parasal karşılığı olabilir, zamansal karşılığı olabilir, fonksiyonel bir karşılık olabilir, kolaylık sağlayabilir. Farklı farklı şeyleri, ne derler, değerler yaratıyor olabilirsiniz burada.\n",
      "\n",
      "Data mining'in data science'a dönüşümünden de biraz bahsetmiştik. Ne kadar veri üretildiğine dair örneklerden konuştuk. Eskiden 3 olan şimdi 5'e çıkan bu 5V'den biraz bahsetmiştik. Hacim hız ve çeşitlilikle aslında ilk tanışmıştık fakat daha sonra doğruluk ve değer kavramlarının dönemi zamanla anlaşılmıştı. Kabataslar bu şekilde bir akışımız oldu. Geriye doğru gittiğimizde, geriye doğru baktığımızda şimdi veri bilim iş akışı üzerinden buradaki adımları biraz böyle detaylıca yani olabildiğince bu ders kapsamında bahsedeceğimiz kadar detaylıca birimi girmeye başlayacağız.\n",
      "\n",
      "Şimdi açıkçası ilk adımımız problemi anlama. Özellikle problemi anlama kısmının ben böyle çok yüzeysel geçildiğini çok yüzeysel bir şekilde evlenmeyi düşünüyorum. Hem gerek farklı eğitimlerde olsun gerek yapılan işlerde olsun aslında. Çoğunlukla nasıl diyeyim şey gibi bir olgunluk yok. İlk olarak bizim problemimiz ne? Biz bunu nasıl çözebiliriz? Bu problemin çözümünü uygulayabilmek için ne gibi elimizde veriler olmalı gibi aşamalardan ziyade birçok yerde şey gibi bir yaklaşım görürsünüz. İşte veri bilimi çok önemliymiş, çok güzelmiş, çok karlı da bir işmiş. Hadi veri bilimine girelim falan. Bizim egzellerimiz var gibi böyle yaklaşımlar görebilirsiniz Türkiye'de. Hatta dünyanın birçok yerinde de görebilirsiniz bunu.\n",
      "\n",
      "Önce gerçekten ilk olarak problemi anlamanın, problem üzerine tartışmanın, brainstorm yapmanın önemini gerçekten atlamamak gerekiyor. Şimdi problemi anlamak için arkadaşlar aslında temel olarak ya şirketin ya da üzerinde çalışacağınız müstakbel projenizin bir amacının tanımlanması gerekiyor. Bu amaç zaten size data scientist olarak hala hazırda belirlenmiş bir şekilde gelebilir. Siz bunu elinizdeki verilerle hem elinizdeki veriden kastım hem iş uzmanlığınızla hem şirketi veya projeyi daha önceden bildiğiniz kadarıyla hem de gerçekten bu bahsettiğimiz gibi işte gerek tabular veri olsun gerek düzensiz veri olsun. Elinizdeki verilerle bu amaca hizmet edip edemeyeceğinizi aslında sorgulamaya ilk andan başlarsınız.\n",
      "\n",
      "Şimdi şirketin veya projenin amacına göre de bir problem seti oluyor. Veya daha önceden yine size belki de belirlenerek gelmiş olan bir problemi tanımlama aşaması oluyor. Yani burada sizin amacınız işte bir banka için bu işi yapıyorsanız riskli müşterilerin belirlenmesi mi? Yoksa daha çok riskli müşterilerin belirlenmesi ise bu riskten kastımız ne? Bankayı kullanmayı bırakacak olan müşteriler mi? Yoksa borçlarını ödememe ihtimali yüksek olan müşteriler mi? Gibi şeyleri tanımlamanız gerekiyor. Problemi paydaşların yani stakeholderları belirlemek. Stakeholder da yine iş dünyasında çok çok karşınıza çıkan bir kavram. Ben onu açıkçası paydaş diye çevirmeyi seviyorum. Daha iyi bir çevirisi olan varsa da hani şu an paylaşırsanız çok sevinirim gerçekten ama bana en makul gelen çeviri paydaş olduğu zamana kadar.\n",
      "\n",
      "Problem paydaşlarını belirlemem. Şimdi evet hani data scientist'in birçok şapkası oluyor. Birçok farklı adımda data scientist yetkinlikleri gerekiyor. Onun dışında en kabaca zaten şöyle bir ayrımla karşılaşacaksınız veya bu ayrımı belki de sorgulamanız gerekecek. Ben data scientist olarak verimi ne derece hazırlayacağım? Yani bana veri hazır olarak gelecek mi? Hazır olarak gelecekse ne kadar hazır olarak gelecek? Hazır olarak gelmeyecekse ben bu veriyi nereden, nasıl alacağım? Kendim ne derece? Çok pardon. Notu alamadım. Ben elimdeki veriyi kendim hazırlayacaksam da projenin bu aşamalarını da benim bir şekilde belirlemem gerekiyor. Ortaya koymanız gerekiyor.\n",
      "\n",
      "Tabi bu işi açıkçası yazılım dünyasına biraz daha bekende doğru bakan bir yüzü. Bir de işin frontende bakan bir yüzü oluyor. Yani sizin yaptığınız işi kullanacak olan insanlar insanlarla da bir diyalog kurman gerekiyor g sonunda siz harika modeller harika deploymentlar yapabilirsiniz ama günün sonunda bu özellikle projenin amacına uygun değerler üretip üretmeyeceğinizinde birilerinin aynı zamanda bir crosscheck yapması gerekiyor. Bu da yine önemli bir paydaş noktası. Probleme ait veriyi tanımlamanız gerekiyor. Yani aslında nasıl bir şekilde başladık? Bizim de amacımız olacak işte karlılığı arttırmak mı, zararları azaltmak mı, süreçleri hızlandırmak mı vs. Bu amaç için çözmemiz gereken problemler neler? Bu problemleri kimler çözecek? Bu problemleri çözecek olan kişiler hangi veriler üstünde çalışacak? Bunları belirlemeniz gerekiyor. Bunlar hakikaten önemli kısımlar.\n",
      "\n",
      "Bazen özellikle bazı böyle kompleks proje akışlarında bu sular biraz karışır. Ama siz olabildiğince en azından kendi kafanızda bunu oturtacak diyalogları da kurabilirsiniz daha doğrusu. O kadar daha rastlılığa işinizi yaparsınız. Çünkü günün sonunda buradaki cevaplanmamış bazı sorular sizin veri hazırlığınızı bile bazen baştan almanıza sebep olabiliyor arkadaşlar. İletişim kurmaktan kaçınmamamız gerekiyor data scientist olarak ve iletişim kurarken olabildiğince açık, odaklı, gerçekçi ve küçük parçalara bölerek aslında iletişim kurmayı ben tavsiye ediyorum. Çünkü günün sonunda gerçekten açık bir şekilde iletişim kurmazsanız gerçekten farklı beklentiler insan beraber çalıştığınız veya karşılıklı çalıştığınız iş arkadaşlarınız bazen farklı beklentilere girebiliyorlar ya düşük beklentilere girebiliyorlar ya yüksek beklentilere girebiliyorlar günün sonunda sizin bu işin hem matematiğini hem kodlamasını bilen ve yapabilen kişi olarak bir yandan da bunu açıkça ifade edebilmeniz gerekiyor.\n",
      "\n",
      "Odalı iletişim kurmanızı tavsiye ediyorum çünkü zaten üzerinde çalıştığınız konu olabildiğince popüler ve şu anda öyle hype'de olan bir konu ve insanlar böyle gerçekten bir anda böyle egzerlerden böyle işte genelleşmiş yapay zekalara falan uçakmış gibi hissedebiliyorlar ve bunu farkında olmadan yapıyorlar. Dolayısıyla sizin orada odayı korumanız açıkçası sizin açınızdan çok önemli oluyor. Açıklıklı gerçekçilik zaten biraz böyle paralel noktalar olduğunu düşünüyorum. Küçük parçalara bölme konusu da iletişime geçtiğiniz her paydaş sizinle veya başka paydaşlarla aynı bilgi noktasında olmayabilir. Yani gerek dikey olarak yani bilgi seviyesi olarak gerek de bilgi çeşitliliği olarak yatay olarak da yani yatay yararçıda da farklı açılardan bakan insanlarla çalışıyor olacaksınız. Dolayısıyla bu problem çözme aşamın, bu problem çözme sürecinizi daha doğrusu küçük parçalara doğru bir şekilde bölüp doğru insanlara doğru iletişimler kurarak sürecinizi yönetmeniz gerekiyor. Bunun için de tabi olabildiğince soru sormanız gerekiyor. Kendi yaptığınız işe yeterince hakim olursanız hem yaratıcı hem esnek olabilirsiniz.\n",
      "\n",
      "Öte yandan da gerek ışın kılıcıyla ekmek kesmemek için ya da olmayacak işlere olur denemek için de literatür taramanızı ben sıkça tavsiye ediyorum. Bu arada böyle bu konuştuğumuz konularla ilgili böyle sorusu veya böyle aktarmak istediği tecrübesi olan varsa onları konuşabiliriz. Çok güzel olur. Var mı katılmak isteyen? Buradaki literatür taraması biraz daha şey gibi oluyor değil mi? Elimize bir proje geldi. O projeyle ilgili aslında daha önce neler yapılmış, ne yazılar yazılmışla bağlantılı herhalde. Ben şu anda çünkü yaptığım iş product menajerlik yapıyorum ve product menajerlikte da aslında benzer bir süreç oluyor. Çünkü bazen bazı projelerde bunu böyle yapalım diyorlar ya da işte örnek veriyorum bir register sayfasında işte split screen yapalım diyorlar. Ama mesela bir gidiyoruz literatür taraması yapıyoruz ki işte split screen genellikle başka başka sayfalarda kullanılıyormuş. Register'da önerilmiyor. Önerilirse işte şu oluyor bu oluyor gibi şeyler çıkıyor karşımıza. O durumda da aslında biraz daha böyle şey elimiz daha dolu geri dönüyoruz. Hani yaparız ama bakın literatür bunu söylüyor. Ve eğer bunu yaparsak belki işte örnek veriyorum register'da yüzde şu kadar user'ı direkt burada drop off olur. İşte kaybederiz falan diyebiliyoruz. Bu tarz bir şey herhalde buradaki de değil mi?\n",
      "\n",
      "Evet evet. Mesela bu gerçekten çok güzel bir örnek oldu. Valla çok teşekkür ederim. Hakikaten çok güzel bir örnek. Çünkü günün sonunda Amerika'yı bir daha teşrif etmeye gerek yok veya orada bir Amerika olmadığına da bir daha teşrif etmeye gerek olmaması gerekiyor bazen. Ya da bazen gerçekten orada bir Amerika varsa da onu belki de daha da geniş geliştirebileceğimiz ipuçlarını arayabiliyoruz literatürde. Gerek işte sizin söylediğiniz gibi böyle farklı use case'lerle gerek böyle çok basitçe mesela hakikaten hiç literatürde diyelim ki bulamadınız veya o an böyle işte paper okuyasınız yok, pdf'lerde boğulasınız yok. En kötü bir başlangıç noktası olarak mesela Kaggle'ı deneyebilirsiniz. Mesela çok hani ufak bir ipucu olarak bunu söyleyebilirim. Çok teşekkür ederim Yeşim. Rica ederim.\n",
      "\n",
      "Şimdi problemi anlamayla ilgili açıkçası başka tecrübesini veya sorusunu aktarmak isteyen yoksa veri toplamayla devam edeceğim. Veri toplamanın açıkçası zaten çok kabaca böyle şeyleri belli, adımları belli. İlk olarak hakikaten sizin bir veri kaynağınızı tespit etmeniz gerekiyor. Birinci ilmi, ikinci il verimi. Birinci ve ikinci il veriyi herkes hatırlıyor mu? Bunu şöyle sorayım. Hiç hatırlamayan var mı birinci veri ikinci veri nedir diye yok sanırım tamam yani işte herkes hatırlıyorsa hızlıca devam edeyim burada siz burada birinci veri kaynağı mı kullanacaksınız, ikinci veri kaynağı mı kullanacaksınız? Bunun tespitini veya tercihini de bulunmanız gerekiyor. Sizin araştırma sorunuz yani hipoteziniz ne? Bunu baştan belirlemeniz gerekiyor ki veri toplamayla hipotez belirlemenin bazen ilişkisi çok net kurulamayabiliyor. Fakat günün sonunda siz asl hipotezinize göre veri toplaman gerekiyor veya toplanm veriler aras yine baz b bir se ge yapman gerekiyor.\n",
      "\n",
      "Sizin hedef popülasyonunuz nedir? Buna bağlı olarak örnekleriniz nedir? Günün sonunda siz tüm Türkiye hakkında bir çıkarım yapmak istiyor olabilirsiniz. Fakat bunun için örnekleriniz yeterli mi değil mi? Hem nijel olarak hem de nitelik olarak işte popülasyonumuzu ne kadar temsil ediyor, eksikleri ne, fazlaları ne, yine bunlar hakkında da belli bir olgunlukta fikrinizin olması gerekiyor. Özellikle şu an böyle daha çok üzerine konuştuğumuz ikinci veriyle yapılan yani bizim işte gidip birebir deney gözlem yaparak değil, işte anket doldurarak değil, bizim eylemlerimiz sonucu dijital dünyada hakkımızda oluşan veriler üzerinden yapılacak bir veri toplama çalışmanız varsa sesim geliyor mu? Evet geliyor. Görüntü mü dondu? Bende görüntü de düzgün. Bende de ses ve görüntü düzgün. Tamam. Allah Allah. Ben kendi monitörümü dondurdum. Neyse. Sistematik ve sürekli olmaktan bahsedecektim. Şimdi eğer ikincin veri kaynaklarıyla çalışacaksınız sizin veri toplama sürecinizin sistematik ve sürekliliği konusunda emin olmanız gerekiyor arkadaşlar ki kurduğumuz veri bilimi iş akışı sürekli çalışmaya devam edebilsin. Yani topladığınız verilerin güncelliğinden sizin bir şekilde sistematik ve sürekli olarak emin olmanız gerekiyor.\n",
      "\n",
      "Arkadaşlar çok pardon. Şu an slaytım değişiyor mu? Az önce değişmişti. Şu an değişti. Olası veri kaynakların. Tamamdır. Nasıl yapsam? Birinci veri kaynakları ile ilgili zaten onun kaynağını bir nevi kendimiz yaratıyoruz diyebiliriz. Burada bahsettiğim başçası arkadaşlar olası veri kaynaklarından web siteleri, akademik makaleler, Amazon Türk gibi ticari veri sağlayıcılar, online veri tabanları ve sosyal medya gibi kaynaklar aslında bizim için böyle olası veri kaynakları. Burada dikkat etmemiz gereken şöyle bir nokta oluyor. Topladığımız veri her zaman kaliteli bir veri olmayabiliyor. Kaliteli bir veri olmayabiliyor. Veri kalitesinin birçok farklı katmanı var aşaması var diyebilirim ama şimdilik onu çok detayına girmeyelim. Topladığımız veriler bizim az önce üzerinde konuştuğumuz araştırma sorunuza yani hipotezimizle ne kadar alakalı o konudan gerçekten emin olmamız gerekiyor çünkü birinci verinin aksine ikinci veride böyle nasıl diyeyim bir yoğun olarak toplanmış veri kümeleriyle aslında ilk başta karşı karşıya kalıyoruz. Bunları bizim gerçekten ciddi bir veri ön işleme sürecinden geçirmemiz gerekiyor. Bu işin tabii ki bir maliyeti var ve aynı zamanda bir zaman maliyeti de var. Bu veri toplama işinin ne kadar maliyetli ve ne kadar zaman alacağını da hakikaten olabildiğince böyle önden belki de küçük testlerle biraz böyle simülasyonlarını yapmakta fayda olabiliyor. Çok sistematik bir sürekli Twitter'dan veri kılavuz eden bir sisteminiz varsa mesela bunun böyle sürekliliğini sürdürülebilirliğini cidden önden biraz test etmenizde fayda var. Ve bu veriye erişimin kolaylığı. Veri erişim kolaylığının iki aslında yüzü var. Bir tanesi sizin veri bilimi akışınız veriye ne kadar kolay erişebiliyor. Yani siz bu sistemi ne kadar akıcı kurdunuz? İkincisi de aslında sistemin güvenliğiyle ilgili. Bununla ilgili de artık birçok farklı böyle çalışmalar yapıldığını siz de şahit olmuşsunuzdur. Açıkçası hani ben de şahit oluyorum böyle eskiden bu konulara çok özen gösterilmezken şimdi böyle işte ne derler hani veri maskeleme üzerine veri maskelemeler vesaire işte tool factor işte authenticationlar falan gibi böyle şeyler artık çok sıradan hale gelmeye başladı. Dolayısıyla hani veri toplamanın 3 aşağı 5 yukarı arkadaşlar böyle hani önemli kısımları bunlar yani ilk olarak hani hipotezimize uygun sistematik ve sürekli veri toplayabileceğimiz bir sistem oluşturmaya çalışıyoruz. Ve bu oluşturduğumuz sistemden gelen verilerin de bizim kurduğumuz eğer özellikle yapay öğrenme yani machine learning algoritmaları kullanıyorsak bu verinin kalitesi ve güvenlik ve akıcılık açısından da bu veri erişimine kolaylığının sağlanması gerekiyor.\n",
      "\n",
      "Veri toplamayla ilgili de açıkçası hani ilk başta en azından bu seviyede konuşacaklarımız bunlar. Bununla ilgili böyle bir sorusu olan veya paylaşmak istediği bir tecrübesi olan var mı? Özellikle veri toplamaya dair. Hocam benim burada sormak istediğim husus şu. Şimdi problemi tanımladık ve tanımladığımız o probleme uygun veri toplamaya başlayacağız ama eğer ki probleme uygun veri ulaşamazsak orada nasıl bir çözüme gideceğiz? Yani ulaşılabilecek bir veri yoksa biz mi yaratmalıyız? Yani böyle işte birincil veri kaynaklarına mı gitmeliyiz orada? Yani tabii önce şey nasıl diyeyim ya açıkçası sentetik veri benim böyle nasıl diyeyim çok böyle mantıklı bulamadığım bir şey. Günün sonunda. Bu arada bu taraftaki monitörün sıkıntı var o yüzden ben buraya dönüyorum arkadaşlar. Kusura bakmayın. Sentetik veriden ise olabildi birinci veriye veri taraf yat yapmak bana mant geliyor durumlarda Çünkü hakikaten şey günün sonunda az önce o bahsettiğin işte hali hazırdaki veri yokluğu eğer birinci veri tarafına yeteri kadar dönülecek bazen bu yatırım boyutlarına ulaşılabiliyor çünkü birinci veri tarafına ayrılabilecek bir vakit ve kaynak varsa hakikaten o dezavantaj böyle avantaja dönüşüyor ve gerçekten çok kriteri verilerle nasıl diyeyim çok uzun süreler güzel çalışabilecek sistemlerin verisi aslında toplanmış oluyor. Onu tavsiye ederim. İşin sentetik veri kısmına gelince de o da tamamen faydasız bulmuyorum aslında. O da en azından sizin nasıl bir şeyle karşılaşacağınızı hem bu işin yazılımını geliştirirken özellikle nasıl bir şeyle karşılaşacağınızı veya nasıl bir veri size gelirse daha iyi olurum size böyle cevaplarını büyük ölçüde sağlıyor. O açıdan güzel ama sonuçları itibariyle hakikaten gerçek dünyadan çok kopuk şeyler yaratabiliyor sentetik veriler. Ama özellikle yazılım tarafının yürütülmesini gerçekten nasıl diyeyim güzel bir emek oluşturuyor diyebilirim yani. Teşekkür ederim. Ben teşekkür ederim. Başka paylaşmak istediği sorusu veya tecrübesi olan var mı veri toplamaya dair?\n",
      "\n",
      "Benim şu anda yaşadığım bir şey var aslında burada. Ya şu anda bir tez hazırlıyorum ben. Kumaşların sürdürülebilirliğiyle alakalı. Bunun için benim için gerekli olan şey önceden staj yaptım. Bir yerde Kumaşlardan artan kumaşların kilogram miktarları. Tabii ki bunu elde etmek biraz zordu çünkü artık orada çalışmıyorum. Ama gelen veriyi bir şekilde aldım. Bu sefer o kumaşları kilograma çevirmem gerekiyordu. Ayrıca bunun üstüne çok fazla hatalı girilen yerler vardı veri akışını bozan. Ben bunu analiz ettiğimde veriyi bozan çok fazla şey vardı. Böyle bir örnek vermek istedim. İşte bunların ayıklanmasının mesela bir yöntemleri var ama ben yani elimle biraz amele bir şekilde yaptım açıkçası. Valla şey ne derler yeri geldiğinde gerçekten bazen böyle tek tek yapmak zorunda kalıyor insan. Yani o konuda nasıl diyeyim yani çok böyle otomatize çözümler hakikaten enerjikçisi ben de henüz rastlamadım. Özellikle hani bahsettiğin iş veya proje baya şey ne derler custom bir iş gibi geldi bana. Dolayısıyla şey biraz böyle hani el emeği göz nuru diyebileceğim bir veri oluşturmuşsun. Ben en azından eline sağlık diyeyim. Hani baya zordur diye tahmin ediyorum. Teşekkürler. Ben teşekkür ederim.\n",
      "\n",
      "Şimdi veri hazırlamadan biraz bahsedelim. Veri hazırlamanın hedefinde arkadaşlar topladığımız veriyi analize uygun hale getirmek var aslında. Yani bunun için de elimizdeki veriyi biz nasıl bir hale getirmek istiyoruz? Bizim hipotezimizle daha alakalı bir hale getirmek istiyoruz. Daha doğru, daha güvenilir ve daha kullanışlı bir hale getirmek istiyoruz günün sonunda. Tabii bunu yapabilmek için yapacağımız aday analizleri 3 aşağı 5 yukarı iyi öndermemiz gerekiyor veya bunları iyi hazırlanmamız gerekiyor. İşte atıyorum ben bir regresyon mu yapacağım, sınıflandırma mı yapacağım, kümeleme mi yapacağım, nebaz da yapacağım, işte farklı işte anovati test gibi böyle basit yani görece basit istatistiki testler deneyecek miyim vesaire bunları deneyeceksem elindeki verinin deseni nasıl olmalı gibi gibi böyle bunları biraz böyle hem yapacağımız analizleri hem kurduğumuz hipotezi yani araştırma sorumuzu iyi düşünüp elimizdeki veriyi biraz ona göre hazırlamamız gerekiyor İşte bu kimi zaman elinizdeki veriyi mesela literatürde çok görürsünüz işte veriyi işte long tam short'a almak, işte long tam wide'a almak, wide tam long'a almak. Bu ne demek? Aslında pivot tablosu yapmak gibi. Hani elimdeki veriyi ben pivot tablosuna uygun hale git, hani uygun halde nasıl hazırlayabilirim? Veya ben elimdeki veriyi pivot tablosu gibi işlemler yaparak yine acaba veri hazırlığı aşamasında adım atabilir miyim gibi şeyleri öngörmek gerekiyor biraz. Tabii veri hazırlamanın Ama bazı böyle şey kısımları var. Ne derler hani? Kendine has kısımları var. Bunlardan ilki veri temizliği. Şimdi veri temizliğini neden yaparız? Yani ya tekrarlı veriniz vardır bizim işte duplicate dediğimiz. Yani işte atıyorum aynı satırdan işte iki üç tane tekrarlamış veya işte farklı farklı satırlardan böyle kendini tekrar edenler var. Bunları tespit etmeniz gerekiyor. Ki hani bunlar özellikle yine o öğrenme algoritmalarında böyle sıkıntılara sebep olabiliyor diyebilirim. Yanlış gerilen veriler vardır. Az önce Ayşe Betül'ün söylediği gibi hani işte mesela çok klasik örnektir işte. Yaş değeriniz vardır işte elinizdeki veride. İşte iki yüz yaşında bir insan hani veya atıyorum bin dokuz yüz yaşında bir insan muhtemelen olamaz. Hani dolayısıyla o bir hani talp olur muhtemelen. Bir yanlış girilmiş veridir. Onu düzeltmeniz onun farkına varmanız gerekiyor. Eksik veriler olabilir. Yani bu ne demek? İşte bazı işte satır veya sütun bazında bazı hücreler boş kalmış olabilir. Bazı sütunların belki yarıya yakını boş olabilir. Bunlar içinde de bizim imputation dediğimiz eksik değer ataması yapmanız gerekir ya da o satır veya sütunları bir şekilde temizlemeniz gerekir. Ve aynı zamanda da outlier diye bir kavramımız var bizim. O da ayrık değer. Ayrık değerden kastımız ne işte elimizdeki veride ki işte kişilerin kiloları çoğunlukla işte 60 ile işte 80 arasında ama birkaç kişi var ki işte 120-130 kiloda onlar bizim için adlı ayrı oluyor neden elimizdeki bütün kitleden çok farklı bir kilodalar aslında bunların da yine bazı analizlerde bu arkadaşları çıkartmanız gerekir bazı analizlerde olduğu gibi bırakmanız gerekir. Bazı analizlerde onları makul değerlere küçültmeniz gerekir. Kontrollü bir şekilde de hani bu yaptığınız transformasyonu da sizin sisteminiz bir şekilde geri alabilir olursa çok iyi olur tabii ki. Ama bu özellikle ayrık değerlere uygulayacağınız yöntem hakikaten analizinize göre değişen bir yöntem. Ama o konuda ezbere gitmemekte fayda var. İleride bundan sık sık bahsedeceğim. Veri entegrasyonu diye bir kavramımız var. Burada farklı veri setlerini tek bir veri seti haline getirdik.\n"
     ]
    }
   ],
   "source": [
    "edited_transcription = pipeline.invoke({\"transcription\": transcription_text})\n",
    "print(edited_transcription.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
